\documentclass[12pt]{article}
\usepackage{mgates-letter}
\definecolor{dark_blue} {rgb}{0., 0., 0.65}

\usepackage{textcomp}
\usepackage{mathrsfs}  % mathscr font
\usepackage{boxedminipage}
\usepackage{rotating}
\usepackage[inline]{enumitem}
%\usepackage{natbib}
\usepackage{xcolor}
\usepackage[colorlinks, filecolor=dark_blue, urlcolor=dark_blue, linkcolor=black, citecolor=black]{hyperref}

\newcommand{\todoinline}[1]{{\color{violet} #1}}

\begin{document}

\begin{titlepage}

	\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
	\center
	
	\textsc{\Large Ph.D. Programme in Computer Science And Engineering}\\[0.5cm]
	
	\textsc{\Large XXXIX Cycle}\\[0.6cm]
	
	\hrule width \hsize \kern 1mm \hrule width \hsize height 2pt 
	\vspace{0.8cm}
	{ \large \bfseries Ph.D. Thesis Proposal}\\[0.6cm]
	{ \large Engineering Many-Agent Cooperative Learning in Collective Adaptive Systems }\\[0.6cm]

	\bfseries{February, 2024}


    \vspace{1.5cm}
    
    \noindent
    \begin{minipage}[t]{0.45\textwidth}
        \raggedright
        \textbf{Supervisors:}\\[0.5cm]
        Prof. Mirko Viroli\\
        Prof. Danilo Pianini\\
        Prof. Matteo Ferrara
    \end{minipage}%
    \hfill
    \begin{minipage}[t]{0.45\textwidth}
        \raggedleft
        \textbf{PhD Candidate:}\\[0.5cm]
        Davide Domini
    \end{minipage} \\[0.6cm]

	\hrule width \hsize height 2pt \kern 1mm \hrule width \hsize height 1pt
	\vspace{0.4cm}

\end{titlepage}

\section{Research Context}\label{sec:intro}

\paragraph{\emph{Context.}}
Computing devices have become ubiquitous in everyday life.
%
This trend has paved the way for research fields aimed at exploiting
 the potential of device collectives to build next-generation systems, 
 including: collective computing~\cite{DBLP:journals/computer/Abowd16}
 and collective adaptive systems (CAS)~\cite{DBLP:journals/sttt/WirsingJN23,robyphdthesis}.
%
More in particular, following Mitchell's definition~\cite{DBLP:conf/metacognition/Mitchell05}, 
 in this thesis we refer to CAS as distributed systems comprising multiple agents 
 such that each agent:
 \begin{enumerate*}[label=(\roman*)]
	\item can interact with other agents either directly or indirectly;
	\item does not individually posses system-wide knowledge;
	\item can exhibit learning to expand its personal knowledge; and
	\item can make decisions based on collective or aggregated knowledge from some of its peers.
 \end{enumerate*}
%
Notably, we focus on systems involving a large number of agents -- potentially in the hundreds
 or thousands -- which is commonly referred to in the literature 
 as many-agents~\cite{DBLP:phd/ethos/Yang21a}.

\paragraph{\emph{Opportunities and challenges.}}
These systems enable the development of innovative applications in a wide range of real-world domains, such as: 
 smart cities~\cite{DBLP:conf/icse/IftikharRBW017}, 
 traffic control~\cite{DBLP:journals/tits/ChuWCL20,DBLP:books/sp/Muller2011/ProthmannTBHMS11} 
 with autonomous vehicles~\cite{DBLP:journals/corr/BojarskiTDFFGJM16}, 
 coordinated robot swarms for search and rescue~\cite{DBLP:journals/ijon/ZhouLLXS21} 
 or environmental monitoring~\cite{DBLP:conf/acsos/AguzziVE23}, and many more.
%
However, despite this great potential, several challenges arise when engineering these systems.
%
First and foremost, control and decision-making demand particular attention.
%
Achieving an optimal balance between centralized and decentralized control is crucial, 
 as neither extreme is feasible nor desirable in dynamic systems~\cite{DBLP:conf/coordination/CasadeiPVN19}. 
% 
Excessive centralization may lead to bottlenecks and single points of failure, 
 whereas complete decentralization can hinder coordination and consistency.
%
Additionally, the dynamic nature of these systems, characterized by constant 
 environmental changes, mobility, and potential component failures, requires adaptive 
 learning mechanisms capable of responding swiftly 
 to evolving conditions~\cite{DBLP:journals/swarm/PrasetyoMF19}.
%
Another key consideration is the locality principle, where operational efficiency and cost 
 are heavily influenced by the spatial proximity of data sources, processing units, and users.
%
Furthermore, partial observability introduces uncertainty, as individual components may have 
 limited or incomplete information about the global state, complicating accurate 
 decision-making~\cite{DBLP:conf/uai/HeDB22}.
%
Data privacy is also a growing concern, particularly in light of stringent regulations like GDPR~\cite{GDPR}
 in the European Union, necessitating privacy-preserving learning techniques.
%
Finally, data heterogeneity~\cite{DBLP:journals/fgcs/MaZLCQ22,DBLP:journals/ijon/ZhuXLJ21},
 stemming from diverse source, can significantly impact learning stability and accuracy.
%
Addressing these challenges is essential for the effective deployment of cooperative learning 
 in collective adaptive systems.

\paragraph{\emph{Related works.}}
In the literature, two main approaches can be identified for developing these systems: 
 those based on manual design and those based on automatic design.

One approach of manual design that has recently gained significant attention is 
 Aggregate Computing (AC)~\cite{DBLP:journals/jlap/ViroliBDACP19}. 
% 
AC is a macro-programming~\cite{DBLP:journals/csur/Casadei23} paradigm that shifts the focus from individual 
 devices to the collective system as a whole, providing a functional programming language to express collective 
 behaviors through computations over distributed data structures 
 known as computational fields~\cite{DBLP:journals/pervasive/MameiZL04,DBLP:journals/jlap/ViroliBDACP19}. 
%
Another relevant approach is multi-agent programming, which involves designing agent-based behaviors 
 using frameworks such as JaCaMo~\cite{boissier2020multi}.

However, engineering collective systems manually can be challenging and error-prone, particularly in complex, 
 non-stationary environments. 
% 
An alternative strategy is to develop methods for the automatic design 
 of behaviors, which can enhance adaptability.
%
There are different options for achieving this, depending on the specific task to be solved. 
%
On the one hand, for unsupervised learning scenarios, a widely adopted approach is to leverage multi-agent 
 reinforcement learning (MARL)~\cite{DBLP:journals/corr/abs-1911-10635,DBLP:journals/tsmc/BusoniuBS08}. 
% 
In MARL, learning is conceptualized as an interaction between multiple agents and their shared environment. 
%
Each agent follows a policy that dictates its actions based on its current state. 
%
The agent then executes the chosen action within the environment. 
%
As a result of these actions, the environment transitions to a new state and provides feedback 
 in the form of rewards to each agent. 
% 
Through this process, agents learn to optimize their policies, ultimately leading to the 
 emergence of complex collective behaviors.
%
On the other hand, for supervised learning scenarios, a common approach is 
 Federated Learning~\cite{DBLP:conf/aistats/McMahanMRHA17}.
%
FL not only enables cooperative learning to train a shared model from distributed datasets
 but also preserves data privacy.
%
This technique allows the agents to train a joint model without the need of collecting and merging
 multiple datasets into a single central server, thus making it possible to work in contexts with strong 
 privacy concerns---for instance, hospitals~\cite{DBLP:journals/csur/NguyenPPDSLDH23} 
 or banks~\cite{DBLP:series/lncs/LongT0Z20}.
%
Moreover, in highly distributed systems, given the large volume of generated data, it becomes inconvenient, 
 or also infeasible, to move all the data to a central server~\cite{DBLP:journals/comsur/NguyenDPSLP21}.
%
In the literature, most FL algorithms share a common learning flow: 
 \begin{enumerate*}[label=(\roman*)]
	\item \emph{model initialization}: the central server initializes a common base model that is shared with each client;
	\item \emph{local learning}: each client performs one or more steps of local learning on its own dataset;
	\item \emph{local models sharing}: each client sends back to the central server the new model trained on its own data 
     (i.e. the local model); and
	\item \emph{local models aggregation}: the local models collected by the central server are aggregated to obtain the 
     new global model.
 \end{enumerate*}
%
This process is carried out iteratively for a predefined number of global rounds. The most common and simple models
 aggregation method is called FedAvg; it was introduced in~\cite{DBLP:conf/aistats/McMahanMRHA17}
 and consists in performing an average of the local models to obtain the next global model.

\paragraph{\emph{Research gap.}}

Despite significant progress in the field of cooperative learning within CAS, 
 several challenges remain unresolved, indicating substantial research gaps.
%
First, achieving large-scale cooperative learning continues to be challenging, both from technological 
 and methodological perspectives. 
% 
Technologically, simulating these systems is highly resource-intensive, requiring significant 
 computational power and time. 
% 
This necessitates the development of more efficient tools and advanced learning pipelines to enhance 
 scalability and reduce resource consumption.
%
Methodologically, learning in environments with a high number of agents is often unstable and challenging to manage. 
%
Centralized learning architectures are frequently impractical, primarily due to privacy concerns or technical 
 constraints related to communication and synchronization overheads. 
% 
Although decentralized approaches have been proposed as a viable alternative, they typically lead to 
 suboptimal performance when compared to centralized counterparts, largely due to difficulties in maintaining global 
 coordination and consistency.
%
Another critical challenge arises from the heterogeneous data distribution among agents.
% 
This heterogeneity is frequently influenced by the spatial distribution of the agents.
%
This builds on the assumption that devices in spatial proximity have similar experiences and make similar 
 observations~\cite{esterle2022deep}, as
 the phenomena to capture is intrinsically context dependent
% 
However, this locality aspect is often overlooked in both MARL and FL literature.


% - despite all the advances in this field several challenges are still open problems
% - first large scale cooperative learning is still difficult to achieve due to both technological and methodological point of view
% - technologically speaking simulating these systems is time and resource consuming, so we need better tools and learning pipelines
% - methodologically, learning with a high number of agents is frequently unstable and difficult
% - a centralized learning is often not feasible (e.g., due to privacy concerns or technical reasons)
% - decentralized approaches are applicable but often leads to worse performances 
% - data distribution among agents are often heterogeneous, and frequently this arises from the spatial distribution of the agents, since it is more likely that nearby agents experience a similar phenomena, but this aspect is often overlooked both in MARL and in FL 

\section{Contribution}\label{sec:contribution}

\paragraph{\emph{Methodological.}}

\paragraph{\emph{Technological.}}


\section{Future Work}\label{sec:future}

	

\section{Thesis Structure}\label{sec:structure}


\bibliographystyle{unsrt}
\bibliography{bibliography}

\end{document}
