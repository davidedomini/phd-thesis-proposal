\documentclass[12pt]{article}
\usepackage{mgates-letter}
\definecolor{dark_blue} {rgb}{0., 0., 0.65}

\usepackage{textcomp}
\usepackage{mathrsfs}  % mathscr font
\usepackage{boxedminipage}
\usepackage{rotating}
\usepackage[inline]{enumitem}
%\usepackage{natbib}
\usepackage{xcolor}
\usepackage[colorlinks, filecolor=dark_blue, urlcolor=dark_blue, linkcolor=black, citecolor=black]{hyperref}

\newcommand{\todoinline}[1]{{\color{violet} #1}}

\begin{document}

\begin{titlepage}

	\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
	\center
	
	\textsc{\Large Ph.D. Programme in Computer Science And Engineering}\\[0.5cm]
	
	\textsc{\Large XXXIX Cycle}\\[0.6cm]
	
	\hrule width \hsize \kern 1mm \hrule width \hsize height 2pt 
	\vspace{0.8cm}
	{ \large \bfseries Ph.D. Thesis Proposal}\\[0.6cm]
	{ \large Engineering Many-Agent Cooperative Learning in Collective Adaptive Systems }\\[0.6cm]

	\bfseries{February, 2024}


    \vspace{1.5cm}
    
    \noindent
    \begin{minipage}[t]{0.45\textwidth}
        \raggedright
        \textbf{Supervisors:}\\[0.5cm]
        Prof. Mirko Viroli\\
        Prof. Danilo Pianini\\
        Prof. Matteo Ferrara
    \end{minipage}%
    \hfill
    \begin{minipage}[t]{0.45\textwidth}
        \raggedleft
        \textbf{PhD Candidate:}\\[0.5cm]
        Davide Domini
    \end{minipage} \\[0.6cm]

	\hrule width \hsize height 2pt \kern 1mm \hrule width \hsize height 1pt
	\vspace{0.4cm}

\end{titlepage}

\section{Research Context}\label{sec:intro}

\paragraph{\emph{Context.}}
Computing devices have become ubiquitous in everyday life.
%
This trend has paved the way for research fields aimed at exploiting
 the potential of device collectives to build next-generation systems, 
 including: collective computing~\cite{DBLP:journals/computer/Abowd16}
 and collective adaptive systems (CAS)~\cite{DBLP:journals/sttt/WirsingJN23,robyphdthesis}.
%
More in particular, following Mitchell's definition~\cite{DBLP:conf/metacognition/Mitchell05}, 
 in this thesis we refer to CAS as distributed systems comprising multiple agents 
 such that each agent:
 \begin{enumerate*}[label=(\roman*)]
	\item can interact with other agents either directly or indirectly;
	\item does not individually posses system-wide knowledge;
	\item can exhibit learning to expand its personal knowledge; and
	\item can make decisions based on collective or aggregated knowledge from some of its peers.
 \end{enumerate*}
%
Notably, we focus on systems involving a large number of agents -- potentially in the hundreds
 or thousands -- which is commonly referred to in the literature 
 as many-agents~\cite{DBLP:phd/ethos/Yang21a}.

\paragraph{\emph{Opportunities and challenges.}}
These systems enable the development of innovative applications in a wide range of real-world domains, such as: 
 smart cities~\cite{DBLP:conf/icse/IftikharRBW017}, 
 traffic control~\cite{DBLP:journals/tits/ChuWCL20,DBLP:books/sp/Muller2011/ProthmannTBHMS11} 
 with autonomous vehicles~\cite{DBLP:journals/corr/BojarskiTDFFGJM16}, 
 coordinated robot swarms for search and rescue~\cite{DBLP:journals/ijon/ZhouLLXS21} 
 or environmental monitoring~\cite{DBLP:conf/acsos/AguzziVE23}, and many more.
%
However, despite this great potential, several challenges arise when engineering these systems.
%
First and foremost, control and decision-making demand particular attention.
%
Achieving an optimal balance between centralized and decentralized control is crucial, 
 as neither extreme is feasible nor desirable in dynamic systems~\cite{DBLP:conf/coordination/CasadeiPVN19}. 
% 
Excessive centralization may lead to bottlenecks and single points of failure, 
 whereas complete decentralization can hinder coordination and consistency.
%
Additionally, the dynamic nature of these systems, characterized by constant 
 environmental changes, mobility, and potential component failures, requires adaptive 
 learning mechanisms capable of responding swiftly 
 to evolving conditions~\cite{DBLP:journals/swarm/PrasetyoMF19}.
%
Another key consideration is the locality principle, where operational efficiency and cost 
 are heavily influenced by the spatial proximity of data sources, processing units, and users.
%
Furthermore, partial observability introduces uncertainty, as individual components may have 
 limited or incomplete information about the global state, complicating accurate 
 decision-making~\cite{DBLP:conf/uai/HeDB22}.
%
Data privacy is also a growing concern, particularly in light of stringent regulations like GDPR~\cite{GDPR}
 in the European Union, necessitating privacy-preserving learning techniques.
%
Finally, data heterogeneity~\cite{DBLP:journals/fgcs/MaZLCQ22,DBLP:journals/ijon/ZhuXLJ21},
 stemming from diverse source, can significantly impact learning stability and accuracy.
%
Addressing these challenges is essential for the effective deployment of cooperative learning 
 in collective adaptive systems.

\paragraph{\emph{Related works.}}
In the literature, two main approaches can be identified for developing these systems: 
 those based on manual design and those based on automatic design.

One approach of manual design that has recently gained significant attention is 
 Aggregate Computing (AC)~\cite{DBLP:journals/jlap/ViroliBDACP19}. 
% 
AC is a macro-programming~\cite{DBLP:journals/csur/Casadei23} paradigm that shifts the focus from individual 
 devices to the collective system as a whole, providing a functional programming language to express collective 
 behaviors through computations over distributed data structures 
 known as computational fields~\cite{DBLP:journals/pervasive/MameiZL04,DBLP:journals/jlap/ViroliBDACP19}. 
%
Another relevant approach is multi-agent programming, which involves designing agent-based behaviors 
 using frameworks such as JaCaMo~\cite{boissier2020multi}.

However, engineering collective systems manually can be challenging and error-prone, particularly in complex, 
 non-stationary environments. 
% 
An alternative strategy is to develop methods for the automatic design 
 of behaviors, which can enhance adaptability.
%
There are different options for achieving this, depending on the specific task to be solved. 
%
On the one hand, for unsupervised learning scenarios, a widely adopted approach is to leverage multi-agent 
 reinforcement learning (MARL)~\cite{DBLP:journals/corr/abs-1911-10635,DBLP:journals/tsmc/BusoniuBS08}. 
% 
In MARL, learning is conceptualized as an interaction between multiple agents and their shared environment. 
%
Each agent follows a policy that dictates its actions based on its current state. 
%
The agent then executes the chosen action within the environment. 
%
As a result of these actions, the environment transitions to a new state and provides feedback 
 in the form of rewards to each agent. 
% 
Through this process, agents learn to optimize their policies, ultimately leading to the 
 emergence of complex collective behaviors.
%
On the other hand, for supervised learning scenarios, a common approach is 
 Federated Learning~\cite{DBLP:conf/aistats/McMahanMRHA17}.
%
FL not only enables cooperative learning to train a shared model from distributed datasets
 but also preserves data privacy.
%
This technique allows the agents to train a joint model without the need of collecting and merging
 multiple datasets into a single central server, thus making it possible to work in contexts with strong 
 privacy concerns---for instance, hospitals~\cite{DBLP:journals/csur/NguyenPPDSLDH23} 
 or banks~\cite{DBLP:series/lncs/LongT0Z20}.
%
Moreover, in highly distributed systems, given the large volume of generated data, it becomes inconvenient, 
 or also infeasible, to move all the data to a central server~\cite{DBLP:journals/comsur/NguyenDPSLP21}.
%
In the literature, most FL algorithms share a common learning flow: 
 \begin{enumerate*}[label=(\roman*)]
	\item \emph{model initialization}: the central server initializes a common base model that is shared with each client;
	\item \emph{local learning}: each client performs one or more steps of local learning on its own dataset;
	\item \emph{local models sharing}: each client sends back to the central server the new model trained on its own data 
     (i.e. the local model); and
	\item \emph{local models aggregation}: the local models collected by the central server are aggregated to obtain the 
     new global model.
 \end{enumerate*}
%
This process is carried out iteratively for a predefined number of global rounds. The most common and simple models
 aggregation method is called FedAvg; it was introduced in~\cite{DBLP:conf/aistats/McMahanMRHA17}
 and consists in performing an average of the local models to obtain the next global model.

\paragraph{\emph{Research gap.}}

Despite significant progress in the field of cooperative learning within CAS, 
 several challenges remain unresolved, indicating substantial research gaps.
%
First, achieving large-scale cooperative learning continues to be challenging, both from technological 
 and methodological perspectives. 
% 
Technologically, simulating these systems is highly resource-intensive, requiring significant 
 computational power and time. 
% 
This necessitates the development of more efficient tools and advanced learning pipelines to enhance 
 scalability and reduce resource consumption.
%
Methodologically, learning in environments with a high number of agents is often unstable and challenging to manage. 
%
Centralized learning architectures are frequently impractical, primarily due to privacy concerns or technical 
 constraints related to communication and synchronization overheads. 
% 
Although decentralized approaches have been proposed as a viable alternative, they typically lead to 
 suboptimal performance when compared to centralized counterparts, largely due to difficulties in maintaining global 
 coordination and consistency.
%
Another critical challenge arises from the heterogeneous data distribution among agents.
% 
This heterogeneity is frequently influenced by the spatial distribution of the agents.
%
This builds on the assumption that devices in spatial proximity have similar experiences and make similar 
 observations~\cite{esterle2022deep}, as
 the phenomena to capture is intrinsically context dependent
% 
However, this locality aspect is often overlooked in both MARL and FL literature.


\section{Contribution}\label{sec:contribution}

\paragraph{\emph{Methodological.}}

% Field-Based coordination for FL
% Proximity based self federated learning
% FBFL: A Field-Based Coordination Approach for Data Heterogeneity in Federated Learning
% IoT Paper? 
% Towards intelligent pulverized systems: a modern approach for edge-cloud services
% Heterogeneous GNN for Collective-Task Offloading in Cloud-Edge via Deep Q-Learning (TAAS)
% Neighbor based decentralized MARL (SAC)
% 

\paragraph{\emph{Technological.}}

% Scarlib
% A scalable simulation pipeline for MARL (DSRT)
% ProFed Benchmark


\section{Future Work}\label{sec:future}

% Transfer Learning + SAC
% Sparse Neural Network
	

\section{Thesis Structure}\label{sec:structure}


\section{Scientific Publications}\label{sec:pubblications}

\sloppypar
\paragraph{\emph{Published or accepted for pubblication.}}

\begin{enumerate}
	\item ScaRLib: A Framework for Cooperative Many Agent Deep Reinforcement 
	 Learning in Scala.~\cite{DBLP:conf/coordination/DominiCAV23}
	\item ScaRLib: Towards a hybrid toolchain for aggregate computing and many-agent 
	 reinforcement learning.~\cite{DBLP:journals/scp/DominiCAV24}
	\item Field-Based Coordination for Federated Learning.~\cite{DBLP:conf/coordination/DominiAEV24}
	\item Towards Intelligent Pulverized Systems: a Modern Approach 
	 for Edge-Cloud Services.~\cite{DBLP:conf/woa/DominiFAV24}
	\item Proximity-based Self-Federated Learning.~\cite{DBLP:journals/corr/abs-2407-12410}
	\item Towards Self-Adaptive Cooperative Learning in Collective Systems.~\cite{DBLP:conf/acsos/Domini24}
	\item A Reusable Simulation Pipeline for Many-Agent Reinforcement Learning.~\cite{DBLP:conf/dsrt/DominiAPV24}
	\item Neighbor-Based Decentralized Training Strategies for Multi-Agent 
	 Reinforcement Learning.~\cite{DBLP:conf/sac/MalucelliDAV25}
\end{enumerate}

\sloppypar
\paragraph{\emph{Submitted for review.}}
\begin{enumerate}
	\item Heterogeneous GNN for Collective-Task Offloading in Cloud-Edge via Deep
	 Q-Learning.~\cite{DBLP:journals/taas/FarabegoliDAV2025}
	\item ProFed: a Benchmark for Proximity-based non-IID Federated Learning.~\cite{DBLP:conf/ijcnn/DominiAV2025}
	\item FBFL: A Field-Based Coordination Approach for Data Heterogeneity in 
	 Federated Learning~\cite{domini2025fbflfieldbasedcoordinationapproach}
	\item Scalable Coordination in Swarms: A Graph Neural Network Approach.~\cite{DBLP:conf/coordination/VenturiniDAV2025}
	\item Decentralized Proximity-Aware Clustering for Self-Federated Learning.~\cite{DBLP:journals/iot/DominiAFVE2025}
\end{enumerate}
\bibliographystyle{unsrt}
\bibliography{bibliography}

\end{document}
